{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "python_image_scraper_methods",
      "provenance": [],
      "authorship_tag": "ABX9TyMTHeoTFViyKYtgxOWbTjM4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BrodySpearman/Python-image-scraping-methods/blob/main/python_image_scraper_methods.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQRjVAs2rU5f"
      },
      "source": [
        "# **Image scraper with Python**\n",
        "\n",
        "# ***Method one: BeautifulSoup with requests to collect html metadata.***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRu_Lz8zrTlx"
      },
      "source": [
        "!pip install requests\n",
        "!pip install bs4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LS6k-No4rk6Z"
      },
      "source": [
        "from bs4 import BeautifulSoup as soup\n",
        "import requests\n",
        "\n",
        "# parameters\n",
        "url = 'https://www.google.com/search?q=abstract+art&client=opera-gx&hs=rdy&sxsrf=ALeKk02DmSL4rU1lcwug_EMwN5Sodd4uHQ:1625774758469&source=lnms&tbm=isch&sa=X&ved=2ahUKEwjo6dn3otTxAhV8Ap0JHVgLDMIQ_AUoAXoECAEQAw'\n",
        "\n",
        "def get_image_data(url):\n",
        "  r = requests.get(url)\n",
        "  return r.text\n",
        "\n",
        "htmldata = get_image_data(url)\n",
        "raw_images = soup(htmldata,'html.parser')\n",
        "\n",
        "for item in raw_images.find_all('img'):\n",
        "  print(item['src'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ATppcmExGOI"
      },
      "source": [
        "# Pros: \n",
        "\n",
        "\n",
        "*   Very readable\n",
        "*   Very simple\n",
        "*   Modular\n",
        "\n",
        "---\n",
        "\n",
        "# Cons:\n",
        "\n",
        "\n",
        "*   Can get messy quick.\n",
        "*   Can only draw off url, rather than key lookup searches. With webdriver however this functionality could be possible.\n",
        "*   Limited: can't scroll, problems when encountering infinite webpages.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySw0E7_Ixwk-"
      },
      "source": [
        "# ***Method Two: Selenium combined with Chrome webdriver.***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4ACNDobwx5f"
      },
      "source": [
        "!pip install selenium\n",
        "!apt-get update \n",
        "!apt install chromium-chromedriver\n",
        "\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "import time\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLc0ie759XRy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5a2bab0-545e-4edc-a678-316d21d86550"
      },
      "source": [
        "# Parameters for scraping\n",
        "query = 'abstract'\n",
        "number_of_images_needed = 50\n",
        "\n",
        "# Needed to work on a jupyter notebook, otherwise would just need to specify a local drive location.\n",
        "chrome_options = webdriver.ChromeOptions()\n",
        "chrome_options.add_argument('--headless')\n",
        "chrome_options.add_argument('--no-sandbox')\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "wd = webdriver.Chrome('chromedriver',chrome_options=chrome_options)\n",
        "driver = webdriver.Chrome('chromedriver',chrome_options=chrome_options)\n",
        "driver.get('https://www.google.com/imghp?hl=en&authuser=0&ogbl') # driver directed towards google images\n",
        "\n",
        "# xpath to the html element corrosponding to search bar\n",
        "box = driver.find_element_by_xpath('//*[@id=\"sbtc\"]/div/div[2]/input')\n",
        "box.send_keys(query) # inputs query into search bar\n",
        "box.send_keys(Keys.ENTER)\n",
        "\n",
        "def auto_scroll():\n",
        "  scroll_height = 'return document.body.scrollHeight'\n",
        "  last_height = driver.execute_script(scroll_height)\n",
        "\n",
        "  while True:\n",
        "    driver.execute_script('window.scrollTo(0, document.body.scrollHeight)')\n",
        "    time.sleep(2)\n",
        "    new_height = driver.execute_script(scroll_height) # Scrolls to bottom of page and resets scroll height\n",
        "    try:\n",
        "      driver.find_element_by_xpath('//*[@id=\"islmp\"]/div/div/div/div/div[5]/input').click()\n",
        "      time.sleep(2)\n",
        "    except:\n",
        "      pass\n",
        "    if new_height == last_height:\n",
        "      break\n",
        "    last_height = new_height # Kind of recursive :)\n",
        "\n",
        "def get_images():\n",
        "  print('finding images...')\n",
        "  for i in range(1, number_of_images_needed):\n",
        "    try:\n",
        "      image = driver.find_element_by_xpath(f'//*[@id=\"islrg\"]/div[1]/div[{str(i)}]/a[1]/div[1]/img')\n",
        "      image.screenshot(f'/content/drive/My Drive/scrape_test/testImage ({str(i)}).png')\n",
        "    except:\n",
        "      pass\n",
        "  print('content downloaded!')\n",
        "\n",
        "auto_scroll()\n",
        "get_images()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: DeprecationWarning: use options instead of chrome_options\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: use options instead of chrome_options\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "finding images...\n",
            "content downloaded!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5i5NR--Wb7uc"
      },
      "source": [
        "# Pros:\n",
        "\n",
        "*   Easy integration and downloads to google drive.\n",
        "*   Can download large amounts of images at a time, great for dataset building.\n",
        "*   Scalable.\n",
        "*   Specifying html data by xpath makes it easy to modify.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "# Cons:\n",
        "\n",
        "*   *Very* messy right now. Images are low resolution and many images contain other residual google elements and white spaces. \n",
        "*   A tad more complicated than method one.\n"
      ]
    }
  ]
}